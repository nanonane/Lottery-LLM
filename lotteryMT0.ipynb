{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 环境配置\n",
    "# 配置python3.9环境\n",
    "# %%capture captured_output\n",
    "!/home/ma-user/anaconda3/bin/conda create -n python-3.9.0 python=3.9.0 -y --override-channels --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "!/home/ma-user/anaconda3/envs/python-3.9.0/bin/pip install ipykernel\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "   \"display_name\": \"python-3.9.0\",\n",
    "   \"env\": {\n",
    "      \"PATH\": \"/home/ma-user/anaconda3/envs/python-3.9.0/bin:/home/ma-user/anaconda3/envs/python-3.7.10/bin:/modelarts/authoring/notebook-conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ma-user/modelarts/ma-cli/bin:/home/ma-user/modelarts/ma-cli/bin\"\n",
    "   },\n",
    "   \"language\": \"python\",\n",
    "   \"argv\": [\n",
    "      \"/home/ma-user/anaconda3/envs/python-3.9.0/bin/python\",\n",
    "      \"-m\",\n",
    "      \"ipykernel\",\n",
    "      \"-f\",\n",
    "      \"{connection_file}\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\"):\n",
    "    os.mkdir(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\")\n",
    "\n",
    "with open('/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/kernel.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "# 注：以上代码执行完成后，需点击左上角或右上角将kernel更换为python-3.9.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装mindspore2.2.14，安装指南详见：MindSpore安装\n",
    "# 安装MindNLP及相关依赖，MindNLP官方仓详见：MindNLP\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.2.14/MindSpore/unified/x86_64/mindspore-2.2.14-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install tokenizers==0.15.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!wget https://repo.mindspore.cn/mindspore-lab/mindnlp/newest/any/mindnlp-0.3.2-py3-none-any.whl\n",
    "!pip install mindnlp-0.3.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoModelForSeq2SeqLM\n",
    "from mindnlp.peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Get the model with a PEFT adapter\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print the trainable parameters of the model\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LoraConfig` specifies how the PEFT adapters should be configured:\n",
    "\n",
    "* `task_type`: Defines the type of task, which in this case is TaskType.SEQ_2_SEQ_LM for sequence-to-sequence language modeling.\n",
    "* `inference_mode`: A boolean that should be set to False when training to enable the training-specific features of the adapters.\n",
    "* `r`: Represents the rank of the low-rank matrices that are part of the adapter. A lower rank means less complexity and fewer parameters to train.\n",
    "* `lora_alpha`: LoRA alpha is the scaling factor for the weight matrices. A higher alpha value assigns more weight to the LoRA activations.\n",
    "* `lora_dropout`: Sets the dropout rate within the adapter layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "\n",
    "To fine-tune the model, let's use the [financial_phrasebank](https://huggingface.co/datasets/takala/financial_phrasebank) dataset. The financial_phrasebank dataset is specifically designed for sentiment analysis tasks within the financial sector. It contains sentences extracted from financial news articles, which are categorized based on the sentiment expressed — negative, neutral or positive.\n",
    "\n",
    "Although the dataset is designed for sentiment classification task, we use it here for a sequence-to-sequence task for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "Load the dataset with `load_dataset` from MindNLP.\n",
    "\n",
    "The data is then shuffled and split, allocating 90% for training and 10% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(25078:139755628070080,MainProcess):2024-07-11-15:30:14.394.536 [mindspore/dataset/engine/datasets.py:1203] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "from data_process import LotteryDataLoader\n",
    "\n",
    "my_dataset = LotteryDataLoader()\n",
    "\n",
    "import mindspore.dataset as ds\n",
    "dataset = ds.GeneratorDataset(my_dataset, [\"enquire\", \"answer\"], shuffle=True)\n",
    "\n",
    "train_dataset, validation_dataset = dataset.shuffle(64).split([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "We then tokenize the text with the tokenizer associated with the mT0 model. First, load the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ros/miniconda3/envs/huawei/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.542 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_name_or_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmindnlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel_name_or_path\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name_or_path' is not defined"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, modify the `BaseMapFunction` from MindNLP to wrap up the tokenization steps.\n",
    "\n",
    "Note that both the `sentence` and the `text_label` columns needs to be tokenized.\n",
    "\n",
    "In addition, to avoid unexpected behavior due to multiple threads attempting to tokenize data at the same time, we use `Lock` from the `threading` module to ensure only one thread can perform the tokenization at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from mindnlp.dataset import BaseMapFunction\n",
    "from threading import Lock\n",
    "lock = Lock()\n",
    "\n",
    "max_length = 128\n",
    "class MapFunc(BaseMapFunction):\n",
    "    def __call__(self, sentence, label, text_label):\n",
    "        lock.acquire()\n",
    "        model_inputs = tokenizer(sentence, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        labels = tokenizer(text_label, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        lock.release()\n",
    "        labels = labels['input_ids']\n",
    "        labels = np.where(np.equal(labels, tokenizer.pad_token_id), -100, labels)\n",
    "        return model_inputs['input_ids'], model_inputs['attention_mask'], labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the map function, shuffle the dataset if necessary and batch the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(dataset, tokenizer, batch_size=None, shuffle=True):\n",
    "    input_colums=['enquire']\n",
    "    output_columns=['answer']\n",
    "    dataset = dataset.map(MapFunc(input_colums, output_columns),\n",
    "                          input_colums, output_columns)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(64)\n",
    "    if batch_size:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 8\n",
    "train_dataset = get_dataset(train_dataset, tokenizer, batch_size=batch_size)\n",
    "eval_dataset = get_dataset(validation_dataset, tokenizer, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now we have the model and datasets ready, let's prepare for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and learning rate scheduler\n",
    "\n",
    "We set up the optimizer for updating the model parameters, alongside a learning rate scheduler that manages the learning rate throughout the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.modules.optimization import get_linear_schedule_with_warmup\n",
    "import mindspore.experimental.optim as optim\n",
    "\n",
    "# Setting up optimizer and learning rate scheduler\n",
    "optimizer = optim.AdamW(model.trainable_params(), lr=1e-3)\n",
    "\n",
    "num_epochs = 3 # Number of iterations over the entire training dataset\n",
    "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=(len(train_dataset) * num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step\n",
    "\n",
    "Next, define the function that controls each step of training.\n",
    "\n",
    "Define `forward_fn` which executes the model's forward pass to compute the loss.\n",
    "\n",
    "Then pass `forward_fn` to `mindspore.value_and_grad` to create `grad_fn` that computes both the loss and gradients needed for parameter updates.\n",
    "\n",
    "Define `train_step` that updates the model's parameters according to the computed gradients, which will be called in each step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import ops\n",
    "\n",
    "# Forward function to compute the loss\n",
    "def forward_fn(**batch):\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "# Gradient function to compute gradients for optimization\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "# Define the training step function\n",
    "def train_step(**batch):\n",
    "    loss, grads = grad_fn(**batch)\n",
    "    optimizer(grads)  # Apply gradients to optimizer for updating model parameters\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Now everything is ready, let's implement the training and evaluation loop and excute the training process.\n",
    "\n",
    "This process optimizes the model's parameters through multiple iterations over the dataset, i.e. multiple epochs, and assesses its performance on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Training loop across epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.set_train(True)\n",
    "    total_loss = 0\n",
    "    train_total_size = train_dataset.get_dataset_size()\n",
    "    # Iterate over each entry in the training dataset\n",
    "    for step, batch in enumerate(tqdm(train_dataset.create_dict_iterator(), total=train_total_size)):\n",
    "        loss = train_step(**batch)\n",
    "        total_loss += loss.float()  # Accumulate loss for monitoring\n",
    "        lr_scheduler.step()  # Update learning rate based on scheduler\n",
    "\n",
    "    model.set_train(False)\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    eval_total_size = eval_dataset.get_dataset_size()\n",
    "    # Iterate over each entry in the evaluation dataset\n",
    "    for step, batch in enumerate(tqdm(eval_dataset.create_dict_iterator(), total=eval_total_size)):\n",
    "        with mindspore._no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(ops.argmax(outputs.logits, -1).asnumpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataset)\n",
    "    eval_ppl = ops.exp(eval_epoch_loss) # Perplexity\n",
    "    train_epoch_loss = total_loss / len(train_dataset)\n",
    "    train_ppl = ops.exp(train_epoch_loss) # Perplexity\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the training loop implementation and understand the key components:\n",
    "\n",
    "* Model Training Mode\n",
    "\n",
    "    Before the training starts, the model is set to training mode by `model.set_train(True)`. Before the evaluation, the training-specific behaviour of the model is disabled by `model.set_train(False)`.\n",
    "\n",
    "* Loss and Perplexity\n",
    "\n",
    "    `total_loss = 0` initializes and `total_loss += loss.float()` accumulates the total loss for each batch within an epoch. This accumulation is crucial for monitoring the model’s performance.\n",
    "\n",
    "    The average loss and the perplexity (PPL), which is a common metric for language models, are reported in the printed message.\n",
    "\n",
    "* Learning Rate Scheduler\n",
    "\n",
    "    `lr_scheduler.step()` adjusts the learning rate after processing each batch, according to the predefined schedule. This is vital for effective learning, helping to converge faster or escape local minima.\n",
    "\n",
    "* Evaluation Loop\n",
    "\n",
    "    During evaluation, in addition to `model.set_train(False)`, `mindspore._no_grad()` ensures that gradients are not computed during the evaluation phase, which conserves memory and computations.\n",
    "    The `tokenizer.batch_decode()` function converts the output logits from the model back into readable text. This is useful for inspecting what the model predicts and for further qualitative analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training\n",
    "Now that we have finished the training, we can assess its performance and save the trained model for future use.\n",
    "\n",
    "### Accuracy compuation and check the predicited results\n",
    "\n",
    "Let's comupute the accuracy of the predictions made on the validation dataset. Accuracy is a direct measure of how often the model's predictions match the actual labels, providing a straightforward metric to reflect the model's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for correct predictions and total predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# List to store actual labels for comparison\n",
    "ground_truth = []\n",
    "\n",
    "# Compare each predicted label with the true label\n",
    "for pred, data in zip(eval_preds, validation_dataset.create_dict_iterator(output_numpy=True)):\n",
    "    true = str(data['text_label'])\n",
    "    ground_truth.append(true)\n",
    "    if pred.strip() == true.strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Calculate the percentage of correct predictions\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "# Output the accuracy and sample predictions for review\n",
    "print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "print(f\"{eval_preds[:10]=}\")\n",
    "print(f\"{ground_truth[:10]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "If you are satisfied with the result, you can save the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "peft_model_id = f\"../../output/{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model for inference\n",
    "\n",
    "Now let's load the saved model and demonstrate how to use it for making predictions on new data.\n",
    "\n",
    "To load the model that has been trained with PEFT, we first load the base model with `AutoModelForSeq2SeqLM.from_pretrained`. On top of it, we add the trained PEFT adapter to the model with `PeftModel.from_pretrained`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.transformers import AutoModelForSeq2SeqLM\n",
    "from mindnlp.peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = f\"../../output/{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "# Load the model configuration\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the pretrained adapter\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, retrieve an entry from the validation dataset, or alternatively create an entry on your own.\n",
    "\n",
    "We tokenize the `'sentence'` in this entry and use it as inputs into the model. Excute it and be curious about what the model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an entry from the validation dataset.\n",
    "# example = next(validation_dataset.create_dict_iterator(output_numpy=True)) # Get an example entry from the validation dataset\n",
    "# print(example['sentence'])\n",
    "# print(example['text_label'])\n",
    "\n",
    "# Alternatively, create your own text\n",
    "example = {'sentence': 'Nvidia Tops $3 Trillion in Market Value, Leapfrogging Apple.'}\n",
    "\n",
    "inputs = tokenizer(example['sentence'], return_tensors=\"ms\") # Get the tokenized text label\n",
    "print(inputs)\n",
    "\n",
    "model.set_train(False)\n",
    "with mindspore._no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10) # Predict the text label using the trained model\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.asnumpy(), skip_special_tokens=True)) # Print decoded text label from the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Union, Optional\n",
    "from typing import List\n",
    "\n",
    "\n",
    "import mdtex2html\n",
    "\n",
    "#将文本中的字符转为网页上可以支持的字符，避免被误认为是HTML标签\n",
    "def parse_text(text):\n",
    "    \"\"\"copy from https://github.com/GaiZhenbiao/ChuanhuChatGPT/\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if line != \"\"]\n",
    "    count = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if \"```\" in line:\n",
    "            count += 1\n",
    "            items = line.split('`')\n",
    "            if count % 2 == 1:\n",
    "                lines[i] = f'<pre><code class=\"language-{items[-1]}\">'\n",
    "            else:\n",
    "                lines[i] = f'<br></code></pre>'\n",
    "        else:\n",
    "            if i > 0:\n",
    "                if count % 2 == 1:\n",
    "                    line = line.replace(\"`\", \"\\`\")\n",
    "                    line = line.replace(\"<\", \"&lt;\")\n",
    "                    line = line.replace(\">\", \"&gt;\")\n",
    "                    line = line.replace(\" \", \"&nbsp;\")\n",
    "                    line = line.replace(\"*\", \"&ast;\")\n",
    "                    line = line.replace(\"_\", \"&lowbar;\")\n",
    "                    line = line.replace(\"-\", \"&#45;\")\n",
    "                    line = line.replace(\".\", \"&#46;\")\n",
    "                    line = line.replace(\"!\", \"&#33;\")\n",
    "                    line = line.replace(\"(\", \"&#40;\")\n",
    "                    line = line.replace(\")\", \"&#41;\")\n",
    "                    line = line.replace(\"$\", \"&#36;\")\n",
    "                lines[i] = \"<br>\"+line\n",
    "    text = \"\".join(lines)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写Gradio调用函数\n",
    "\n",
    "#采用流聊天方式（stream_chat）调用模型，使得生成答案有逐字生成的效果\n",
    "def predict(input, chatbot, max_length, top_p, temperature, history):\n",
    "    chatbot.append((parse_text(input), \"\"))\n",
    "    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,\n",
    "                                               temperature=temperature):\n",
    "        chatbot[-1] = (parse_text(input), parse_text(response))       \n",
    "\n",
    "        yield chatbot, history\n",
    "\n",
    "#去除输入框的内容\n",
    "def reset_user_input():\n",
    "    return gr.update(value='')\n",
    "\n",
    "#清除状态\n",
    "def reset_state():\n",
    "    return [], [], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#运行Gradio界面，运行成功后点击“Running on public URL”后的网页链接即可体验\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">MindNLP ChatGLM-6B StreamChat</h1>\"\"\")\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=4):\n",
    "            with gr.Column(scale=12):\n",
    "                user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=3, container=False)\n",
    "            with gr.Column(min_width=32, scale=1):\n",
    "                with gr.Row():\n",
    "                    submitBtn = gr.Button(\"一键开聊\", variant=\"primary\")\n",
    "                    emptyBtn = gr.Button(\"清除历史\")\n",
    "            with gr.Column(scale=1):\n",
    "                max_length = gr.Slider(0, 4096, value=2048, step=1.0, label=\"Maximum length\", interactive=True)\n",
    "                top_p = gr.Slider(0, 1, value=0.7, step=0.01, label=\"Top P\", interactive=True)\n",
    "                temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)               \n",
    "\n",
    "    history = gr.State([])\n",
    "\n",
    "    submitBtn.click(predict, [user_input, chatbot, max_length, top_p, temperature, history], [chatbot, history],\n",
    "                    show_progress=True)\n",
    "    submitBtn.click(reset_user_input, [], [user_input])\n",
    "\n",
    "    emptyBtn.click(reset_state, outputs=[chatbot, history], show_progress=True)\n",
    "\n",
    "demo.queue().launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huawei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
